ğŸ¦ Real-Time Banking Transactions Pipeline
ğŸ“Œ Overview

This project implements a real-world, end-to-end data engineering pipeline for banking transactions.
It ingests real-time CDC events, persists them as partitioned Parquet files, and transforms them into analytics-ready dimensional models using dbt and Snowflake, orchestrated by Airflow.

The goal is to demonstrate production-style data engineering practices, including streaming ingestion, historical tracking (SCD Type 2), containerization, and orchestration.

ğŸ—ï¸ Architecture
<div align="center">
  <img src="./diagram.png" />
</div>


ğŸ”§ Tech Stack

PostgreSQL â€“ Source transactional banking database

Apache Kafka â€“ CDC event streaming

Python â€“ Kafka consumers & Parquet data writer

Parquet + S3/Object Storage â€“ Partitioned data lake storage

Apache Airflow â€“ Workflow orchestration & scheduling

Snowflake â€“ Cloud analytics data warehouse

dbt â€“ Data transformations & dimensional modeling

Docker â€“ Containerized execution environment

Power BI (DirectQuery) â€“ Real-time analytics dashboards

ğŸ“¥ Data Pipeline Architecture
ğŸ”„ Ingestion Layer

Banking data captured from PostgreSQL via CDC.

Events streamed through Kafka topics:

transactions

customers

accounts

Python consumers buffer and write data into:

Partitioned Parquet files

Stored in S3/Object Storage

Partition structure:

entity/year/month/day/hour/batch.parquet

â±ï¸ Orchestration (Airflow)

Airflow DAGs manage the full pipeline:

Load Parquet files from S3 â†’ Snowflake RAW layer

Trigger dbt transformations

Execute snapshots & marts

Handle retries and scheduling

â„ï¸ Snowflake Data Modeling (dbt)
1ï¸âƒ£ RAW Layer

Data ingested directly from S3

Represents near-source structured tables

2ï¸âƒ£ Staging Layer (stg_)

Data cleaning & type casting

Naming standardization

Transformations

stg_transactions

stg_customers

stg_accounts

3ï¸âƒ£ Dimensional Models

Built using dbt best practices:

Dimension Tables

dim_customers

dim_accounts

Fact Tables

fact_transactions

Includes:

Historical tracking via SCD Type 2

Analytics-optimized schema

Business-ready datasets

ğŸ“Š Analytics Layer â€“ Power BI

Power BI connects to Snowflake using DirectQuery

Enables near real-time dashboards

No data duplication

Supports operational and analytical reporting

âœ… Project Completion Status

âœ” Real-time CDC ingestion
âœ” Kafka streaming pipeline
âœ” Parquet data lake layer
âœ” Airflow orchestration
âœ” Snowflake RAW ingestion
âœ” dbt staging models
âœ” SCD Type 2 snapshots
âœ” Dimensional models (Customers & Accounts)
âœ” Fact transactions model
âœ” Power BI DirectQuery dashboards

ğŸ¯ Outcome

This project demonstrates a full enterprise-grade modern data stack, covering:

Real-time data engineering

Lakehouse architecture

Analytics engineering with dbt

Workflow orchestration

Dimensional modeling

Production-style containerized systems

Business intelligence delivery
