ğŸš€ Real-Time Banking Data Pipeline

Production-style real-time data pipeline simulating modern banking transaction processing using CDC, streaming, cloud storage, transformation layers, and orchestration.

This project is designed to mirror how data engineering teams build, monitor, and debug real-world financial data pipelines.

ğŸ“Œ Project Overview

This pipeline ingests real-time banking transactions, streams them through Kafka using CDC (Debezium), stores raw and curated data in a lakehouse-style architecture (Bronze / Silver / Gold), and transforms it using dbt, orchestrated by Apache Airflow.

The focus is not just data flow â€” but reliability, observability, and failure recovery, exactly how pipelines behave in production.

ğŸ— Architecture
![Uploading Untitled Diagram.drawio (3).pngâ€¦]()
ğŸ›  Tech Stack
Category	Tools
Language	Python
Streaming	Apache Kafka
CDC	Debezium
Storage	S3 / MinIO
Transformation	dbt
Orchestration	Apache Airflow
Data Warehouse	Snowflake / Postgres
Containerization	Docker
CI/CD	GitHub Actions
Visualization	Power BI / Metabase
ğŸ§  Key Engineering Concepts Demonstrated

Change Data Capture (CDC)

Event-driven streaming pipelines

Bronze / Silver / Gold data modeling

Fault-tolerant ingestion

Retry & recovery strategies

Data quality enforcement

Production logging & debugging

ğŸ”„ Data Flow (Bronze â†’ Silver â†’ Gold)
ğŸ¥‰ Bronze Layer (Raw)

Stores unaltered events

Schema-on-read

Replayable for recovery

Source of truth

ğŸ¥ˆ Silver Layer (Cleaned)

Deduplicated records

Standardized schemas

Validated fields

Business-safe data

ğŸ¥‡ Gold Layer (Analytics)

Aggregated metrics

KPI-ready tables

Optimized for BI tools
